@article{reinarzExaHyPEEngineParallel2020,
  title = {{{ExaHyPE}}: {{An}} Engine for Parallel Dynamically Adaptive Simulations of Wave Problems},
  shorttitle = {{{ExaHyPE}}},
  author = {Reinarz, Anne and Charrier, Dominic E. and Bader, Michael and Bovard, Luke and Dumbser, Michael and Duru, Kenneth and Fambri, Francesco and Gabriel, Alice-Agnes and Gallard, Jean-Matthieu and K{\"o}ppel, Sven and Krenz, Lukas and Rannabauer, Leonhard and Rezzolla, Luciano and Samfass, Philipp and Tavelli, Maurizio and Weinzierl, Tobias},
  year = {2020},
  month = sep,
  journal = {Computer Physics Communications},
  volume = {254},
  pages = {107251},
  issn = {00104655},
  doi = {10.1016/j.cpc.2020.107251},
  urldate = {2024-02-15},
  abstract = {ExaHyPE (``An Exascale Hyperbolic PDE Engine'') is a software engine for solving systems of firstorder hyperbolic partial differential equations (PDEs). Hyperbolic PDEs are typically derived from the conservation laws of physics and are useful in a wide range of application areas. Applications powered by ExaHyPE can be run on a student's laptop, but are also able to exploit thousands of processor cores on state-of-the-art supercomputers. The engine is able to dynamically increase the accuracy of the simulation using adaptive mesh refinement where required. Due to the robustness and shock capturing abilities of ExaHyPE's numerical methods, users of the engine can simulate linear and non-linear hyperbolic PDEs with very high accuracy. Users can tailor the engine to their particular PDE by specifying evolved quantities, fluxes, and source terms. A complete simulation code for a new hyperbolic PDE can often be realised within a few hours {\textemdash} a task that, traditionally, can take weeks, months, often years for researchers starting from scratch. In this paper, we showcase ExaHyPE's workflow and capabilities through real-world scenarios from our two main application areas: seismology and astrophysics.},
  langid = {english},
  file = {/home/mivkov/Zotero/storage/6CNXY6L9/Reinarz et al. - 2020 - ExaHyPE An engine for parallel dynamically adaptive simulations of wave problems.pdf}
}

@article{schayeFLAMINGOProjectCosmological2023,
  title = {The {{FLAMINGO}} Project: Cosmological Hydrodynamical Simulations for Large-Scale Structure and Galaxy Cluster Surveys},
  shorttitle = {The {{FLAMINGO}} Project},
  author = {Schaye, Joop and Kugel, Roi and Schaller, Matthieu and Helly, John C. and Braspenning, Joey and Elbers, Willem and McCarthy, Ian G. and {van Daalen}, Marcel P. and Vandenbroucke, Bert and Frenk, Carlos S. and Kwan, Juliana and Salcido, Jaime and Bah{\'e}, Yannick M. and Borrow, Josh and Chaikin, Evgenii and Hahn, Oliver and Hu{\v s}ko, Filip and Jenkins, Adrian and Lacey, Cedric G. and Nobels, Folkert S. J.},
  year = {2023},
  month = oct,
  journal = {Monthly Notices of the Royal Astronomical Society},
  volume = {526},
  number = {4},
  eprint = {2306.04024},
  primaryclass = {astro-ph},
  pages = {4978--5020},
  issn = {0035-8711, 1365-2966},
  doi = {10.1093/mnras/stad2419},
  urldate = {2024-02-15},
  abstract = {We introduce the Virgo Consortium's FLAMINGO suite of hydrodynamical simulations for cosmology and galaxy cluster physics. To ensure the simulations are sufficiently realistic for studies of large-scale structure, the subgrid prescriptions for stellar and AGN feedback are calibrated to the observed low-redshift galaxy stellar mass function and cluster gas fractions. The calibration is performed using machine learning, separately for three resolutions. This approach enables specification of the model by the observables to which they are calibrated. The calibration accounts for a number of potential observational biases and for random errors in the observed stellar masses. The two most demanding simulations have box sizes of 1.0 and 2.8 Gpc and baryonic particle masses of \$1{\textbackslash}times10\^8\$ and \$1{\textbackslash}times10\^9 {\textbackslash}text\{M\}\_{\textbackslash}odot\$, respectively. For the latter resolution the suite includes 12 model variations in a 1 Gpc box. There are 8 variations at fixed cosmology, including shifts in the stellar mass function and/or the cluster gas fractions to which we calibrate, and two alternative implementations of AGN feedback (thermal or jets). The remaining 4 variations use the unmodified calibration data but different cosmologies, including different neutrino masses. The 2.8 Gpc simulation follows \$3{\textbackslash}times10\^\{11\}\$ particles, making it the largest ever hydrodynamical simulation run to \$z=0\$. Lightcone output is produced on-the-fly for up to 8 different observers. We investigate numerical convergence, show that the simulations reproduce the calibration data, and compare with a number of galaxy, cluster, and large-scale structure observations, finding very good agreement with the data for converged predictions. Finally, by comparing hydrodynamical and `dark-matter-only' simulations, we confirm that baryonic effects can suppress the halo mass function and the matter power spectrum by up to \${\textbackslash}approx20\$ per cent.},
  archiveprefix = {arxiv},
  keywords = {Astrophysics - Astrophysics of Galaxies,Astrophysics - Cosmology and Nongalactic Astrophysics},
  file = {/home/mivkov/Zotero/storage/4L3H3HIF/Schaye et al. - 2023 - The FLAMINGO project cosmological hydrodynamical simulations for large-scale structure and galaxy c.pdf;/home/mivkov/Zotero/storage/PR36V9BQ/2306.html}
}

@article{weinzierlTwoParticleingridRealisations2016,
  title = {Two Particle-in-Grid Realisations on Spacetrees},
  author = {Weinzierl, T. and Verleye, B. and Henri, P. and Roose, D.},
  year = {2016},
  month = feb,
  journal = {Parallel Computing},
  volume = {52},
  pages = {42--64},
  issn = {0167-8191},
  doi = {10.1016/j.parco.2015.12.007},
  urldate = {2023-03-21},
  abstract = {The present paper studies two particle management strategies for dynamically adaptive Cartesian grids at hands of a particle-in-cell code. One holds the particles within the grid cells, the other within the grid vertices. The fundamental challenge for the algorithmic strategies results from the fact that particles may run through the grid without velocity constraints. To facilitate this, we rely on multiscale grid representations. They allow us to lift and drop particles between different spatial resolutions. We call this cell-based strategy particle in tree (PIT). Our second approach assigns particles to vertices describing a dual grid (PIDT) and augments the lifts and drops with multiscale linked cells. Our experiments validate the two schemes at hands of an electrostatic particle-in-cell code by retrieving the dispersion relation of Langmuir waves in a thermal plasma. They reveal that different particle and grid characteristics favour different realisations. The possibility that particles can tunnel through an arbitrary number of grid cells implies that most data is exchanged between neighbouring ranks, while very few data is transferred non-locally. This constraints the scalability as the code potentially has to realise global communication. We show that the merger of an analysed tree grammar with PIDT allows us to predict particle movements among several levels and to skip parts of this global communication a priori. It is capable to outperform several established implementations based upon trees and/or space-filling curves.},
  langid = {english},
  keywords = {AMR,Communication-avoiding,Lagrangian-Eulerian methods,Particle sorting,Particle-in-cell,Spacetree},
  file = {/home/mivkov/Zotero/storage/5JESNI8Y/Weinzierl et al. - 2016 - Two particle-in-grid realisations on spacetrees.pdf;/home/mivkov/Zotero/storage/JIXYBPZK/S0167819115001635.html}
}
